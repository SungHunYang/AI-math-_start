{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09. Categorical Cross Entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrQrGOA9bo4nV0PvPzOIJD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SCCE Calculation"
      ],
      "metadata": {
        "id": "D7-3McQwlhPh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcdjJGFHlZ_-",
        "outputId": "7e113875-3431-47fb-f4cb-6daa6e0a8ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 2 4 0 0 2 3 3 1 2 0 4 1 4 3 4], shape=(16,), dtype=int32)\n",
            "SparseCategorical(Tensorflow):  1.9389151\n",
            "SparseCategorical(Manual):  1.938915\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "batch_size, n_class = 16,5\n",
        "\n",
        "predictions = tf.random.uniform(shape=(batch_size,n_class), minval=0, maxval=1, dtype=tf.float32)\n",
        "# 확률, 마지막 layer의 뉴런의 개수는 class의 개수와 같아야 한다.\n",
        "\n",
        "pred_sum = tf.reshape(tf.reduce_sum(predictions,axis=1),(-1,1))\n",
        "predictions = predictions / pred_sum\n",
        "\n",
        "labels = tf.random.uniform(shape=(batch_size,),minval=0, maxval=n_class,dtype=tf.int32)\n",
        "\n",
        "print(labels)  # 이런 식으로 one hot이 아닌 값으로 나올 때는 SparseCategoricalCrossentropy 사용\n",
        "\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "\n",
        "loss = loss_object(labels, predictions)\n",
        "\n",
        "\n",
        "print(\"SparseCategorical(Tensorflow): \",loss.numpy())\n",
        "\n",
        "ce = 0\n",
        "for label,prediction in zip(labels,predictions):\n",
        "  ce += -tf.math.log(prediction[label])\n",
        "\n",
        "ce/= batch_size # 평균\n",
        "print(\"SparseCategorical(Manual): \",ce.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCCE with Model/Dataset"
      ],
      "metadata": {
        "id": "XsJL1_W5lgKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "N,n_feature = 100,2\n",
        "n_class = 5\n",
        "\n",
        "X = tf.zeros(shape = (0,n_feature))\n",
        "Y = tf.zeros(shape=(0,1),dtype = tf.int32)\n",
        "\n",
        "for class_idx in range(n_class):\n",
        "  center = tf.random.uniform(minval =-15, maxval=15, shape=(2,)) \n",
        "\n",
        "  x1 = center[0] + tf.random.normal(shape=(N,1)) \n",
        "  x2 = center[1] + tf.random.normal(shape=(N,1)) \n",
        "\n",
        "  x = tf.concat((x1,x2),axis=1)\n",
        "  y = class_idx*tf.ones(shape=(N,1), dtype=tf.int32)\n",
        "\n",
        "  X = tf.concat((X,x), axis = 0)\n",
        "  Y = tf.concat((Y,y), axis = 0)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "model = Dense(units=n_class, activation='softmax')\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "\n",
        "for x,y in dataset:\n",
        "  predictions = model(x)\n",
        "  loss = loss_object(y,predictions)\n",
        "  print(loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQtsnxwflgWH",
        "outputId": "75de2ca0-d80b-4d37-ddf3-2ce7c545d92b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.047882\n",
            "9.844177\n",
            "9.869303\n",
            "10.027891\n",
            "10.192861\n",
            "9.889158\n",
            "8.303173\n",
            "7.807233\n",
            "8.299297\n",
            "8.09896\n",
            "8.248482\n",
            "8.219882\n",
            "6.4288464\n",
            "4.9762487\n",
            "4.923832\n",
            "4.835242\n",
            "4.928394\n",
            "4.8059626\n",
            "4.322302\n",
            "1.5966445\n",
            "1.5629181\n",
            "1.5330673\n",
            "1.6117269\n",
            "1.4243939\n",
            "1.7527716\n",
            "3.5662096\n",
            "3.5664556\n",
            "3.5032656\n",
            "3.5450354\n",
            "3.4359581\n",
            "3.3738055\n",
            "3.3985813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CCE Calculation"
      ],
      "metadata": {
        "id": "yoDL5pj3lgbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "batch_size, n_class = 16,5\n",
        "\n",
        "predictions = tf.random.uniform(shape=(batch_size,n_class), minval=0, maxval=1, dtype=tf.float32)\n",
        "# 확률, 마지막 layer의 뉴런의 개수는 class의 개수와 같아야 한다.\n",
        "\n",
        "pred_sum = tf.reshape(tf.reduce_sum(predictions,axis=1),(-1,1))\n",
        "predictions = predictions / pred_sum\n",
        "\n",
        "labels = tf.random.uniform(shape=(batch_size,),minval=0, maxval=n_class,dtype=tf.int32)\n",
        "\n",
        "labels = tf.one_hot(labels,n_class)\n",
        "print(labels)  # 이런 식으로 one hot값으로 나올 때는 CategoricalCrossentropy 사용\n",
        "\n",
        "loss_object = CategoricalCrossentropy()\n",
        "\n",
        "loss = loss_object(labels, predictions)\n",
        "\n",
        "\n",
        "print(\"CategoricalCrossentropy(Tensorflow): \",loss.numpy())\n",
        "\n",
        "tmp = tf.reduce_mean(tf.reduce_sum(-labels*tf.math.log(predictions),axis=1))\n",
        "\n",
        "print(\"CategoricalCrossentropy(Manual): \",tmp.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmJg8p9Slgf9",
        "outputId": "e3451066-595e-4b36-c50b-17797d6985db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]], shape=(16, 5), dtype=float32)\n",
            "CategoricalCrossentropy(Tensorflow):  1.4612103\n",
            "CategoricalCrossentropy(Manual):  1.4612103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CCE with Model/Dataset"
      ],
      "metadata": {
        "id": "PaQ8aGzxlgnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "N,n_feature = 8,2\n",
        "n_class = 5\n",
        "\n",
        "X = tf.zeros(shape = (0,n_feature))\n",
        "Y = tf.zeros(shape=(0,),dtype = tf.int32)\n",
        "\n",
        "for class_idx in range(n_class):\n",
        "  center = tf.random.uniform(minval =-15, maxval=15, shape=(2,)) \n",
        "\n",
        "  x1 = center[0] + tf.random.normal(shape=(N,1)) \n",
        "  x2 = center[1] + tf.random.normal(shape=(N,1)) \n",
        "\n",
        "  x = tf.concat((x1,x2),axis=1)\n",
        "  y = class_idx*tf.ones(shape=(N,), dtype=tf.int32)\n",
        "\n",
        "  X = tf.concat((X,x), axis = 0)\n",
        "  Y = tf.concat((Y,y), axis = 0)\n",
        "\n",
        "Y = tf.one_hot(Y, depth=n_class,dtype=tf.int32) # one_hot 인코딩\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "model = Dense(units=n_class, activation='softmax')\n",
        "loss_object = CategoricalCrossentropy()\n",
        "\n",
        "for x,y in dataset:\n",
        "  predictions = model(x)\n",
        "  loss = loss_object(y,predictions)\n",
        "  print(loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFZW3V0UlsXT",
        "outputId": "d8ddc9e9-6993-4b98-dea7-e8932c472e98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.972014\n",
            "4.4569654\n",
            "0.22067007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DX-0LZDoqh9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}